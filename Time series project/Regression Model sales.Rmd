---
title: "regression PHD 191117"
author: "Sai teja Gollapinni"
date: "November 19, 2017"
output: html_document
---

```{r}
rm(list=ls(all=T))
```


```{r}

setwd("C:/Users/saite/Documents/PHD 181117")
getwd()
```

##load the Weather data 
```{r}
library(readxl)
WeatherData_2009=read_excel(path ='C:/Users/saite/Documents/PHD 181117/Day 2 files/WeatherData.xlsx', na = ' ')
WeatherData_2010=read_excel(path = 'C:/Users/saite/Documents/PHD 181117/Day 2 files/WeatherData.xlsx',sheet ='2010', na = ' ')
WeatherData_2011=read_excel(path = 'C:/Users/saite/Documents/PHD 181117/Day 2 files/WeatherData.xlsx',sheet ='2011', na = ' ')
WeatherData_2012=read_excel(path = 'C:/Users/saite/Documents/PHD 181117/Day 2 files/WeatherData.xlsx',sheet ='2012', na = ' ')
WeatherData_2013=read_excel(path = 'C:/Users/saite/Documents/PHD 181117/Day 2 files/WeatherData.xlsx',sheet ='2013', na = ' ')
WeatherData_2014=read_excel(path = 'C:/Users/saite/Documents/PHD 181117/Day 2 files/WeatherData.xlsx',sheet ='2014', na = ' ')
WeatherData_2015=read_excel(path = 'C:/Users/saite/Documents/PHD 181117/Day 2 files/WeatherData.xlsx',sheet ='2015', na = ' ')
WeatherData_2016=read_excel(path = 'C:/Users/saite/Documents/PHD 181117/Day 2 files/WeatherData.xlsx',sheet ='2016', na = ' ')
summary(WeatherData_2009)
str(WeatherData_2009)
sum(is.na(WeatherData_2009))
colSums(is.na(WeatherData_2009))
colSums(is.na(WeatherData_2010))
colSums(is.na(WeatherData_2011))
colSums(is.na(WeatherData_2012))
colSums(is.na(WeatherData_2013))
colSums(is.na(WeatherData_2014))
colSums(is.na(WeatherData_2015))
colSums(is.na(WeatherData_2016))
```
##Reading the holidays and Macro Economics data
```{r}
HolidaysData=read_excel(path = 'C:/Users/saite/Documents/PHD 181117/Day 2 files/Events_HolidaysData.xlsx',sheet = 'Holidays_count', na = ' ')
EconomicData=read_excel(path = 'C:/Users/saite/Documents/PHD 181117/Day 2 files/MacroEconomicData.xlsx', na = ' ')
colSums(is.na(HolidaysData))
colSums(is.na(EconomicData))
nrow(HolidaysData)
Holidays_train= read.csv(file = 'Holiday Train.csv', sep = ",", header =T)
str(Holidays_train)
summary(Holidays_train)
Holidays_test= read.csv(file = 'Holiday Test.csv',sep = ',', header = T )
str(Holidays_test)
summary(Holidays_test)
Eco_train=read.csv(file = 'Eco_Train.csv', sep = ',', header = T)
Eco_test=read.csv(file = 'Eco_Test.csv', sep = ',', header = T)
Weather_train= read.csv(file = 'Weather_Train.csv', sep = ',', header = T)
Weather_test= read.csv(file ='Weather_Test.csv', sep = ',', header = T )
WeatherEco_train = cbind(Weather_train, Eco_train)
WeatherEcoHoli_train=cbind(WeatherEco_train,Holidays_train)
Train_whole= WeatherEcoHoli_train
```
##sales Data
```{r}
salesData =read.csv(file = 'Train.csv', sep = ',', header = T)
sales_Female =salesData[which(salesData$ProductCategory == "WomenClothing"),]
sum(is.na(sales_Female))
names(Train_whole)[2] = "Year"
names(Train_whole)[1] = "Rank"
Data_model=cbind(Train_whole,sales_Female$Sales.In.ThousandDollars.)
names(Data_model)
Data_model= Data_model[,-c(38,39)]
#rm(data_train)
#rm(data_test)
names(Data_model)
names(Data_model)[39] = "SalesinThousandDollars"
str(Data_model)
names(Data_model)
data_test= Data_model[which(Data_model$Year ==2015),]
data_train= Data_model[which(Data_model$Year!=2015),]
names(data_train)
names(data_test)
data_test= data_test[,-c(2)]
data_train= data_train[,-c(2)]
sum(is.na(data_train))
sum(is.na(data_test))
##Imputation
library(DMwR)
train_imputed=centralImputation(data_train)
complete.cases(data_train)
train_kimp=knnImputation(data_train,k = 3)
sum(is.na(train_imputed))
test_imputed=centralImputation(data_test)
complete.cases(data_test)
test_kimp=knnImputation(data_test, k=5)
#names(train_imputed)
#rm(trainimputed_withouttar)
#rm(testimputed_withouttar)
trainimputed_withouttar=train_imputed[,-c(38)]
train_target=train_imputed$SalesinThousandDollars
testimputed_withouttar=test_imputed[,-c(38)]
test_target=test_imputed$SalesinThousandDollars
##standarization of the whole data without sales
library(vegan)
traindata_stand=decostand(trainimputed_withouttar,"standardize")
testdata_stand=decostand(testimputed_withouttar,"standardize")
final_test=cbind(testimputed_withouttar,test_target)
final_train=cbind(trainimputed_withouttar,train_target)
names(final_train)[38]="sales"
names(final_test)[38]="sales"
str(Data_model)

```


###Train and Test lm
```{r}

Final_lm=lm(sales ~ ., data = final_train)
summary(Final_lm)
predict <- predict(Final_lm,final_test)
#predict <- predict(model,test)

MAPE_train_lm <- mean(abs(test_target-predict)/abs(test_target))*100

library(MASS)
model_aic = stepAIC(object = Final_lm,direction = "both")
summary(model_aic)
Zpredict = predict(object = model_aic,newdata = final_test,type = "response")
MAPE_train_aic <- mean(abs(test_target-Zpredict)/abs(test_target))*100

#Modal Comparision
MAPE_train_lm
MAPE_train_aic
#PredictFor_model_data prediction via lm
model = lm(formula = train_target ~ . , data = trainimputed_withouttar)
model_aic = stepAIC(object = model,direction = "both")

predictForSumbmission <- predict(model_aic,predict)
write.csv(predictForSumbmission,"prediction.csv")
```


````{r}
library(h2o)

localH2O <- h2o.init(nthreads = -1,enable_assertions = F)

train = cbind(final_train,train_target)
train_H2o<- cbind(final_train,train_target)
h2o.init()
train.h2o <- as.h2o(train)
test.h2o <- as.h2o(test)
#for Prediction
predict.h20 <-as.h2o(PredictFor_model_data)
trainTill2015ForH2o.h20 <-as.h2o(train_H2o)

colnames(train.h2o)
dependent_var = 12

rf = h2o.randomForest(y = dependent_var,training_frame = train.h2o,ntrees = 1000,mtries = 10,seed = 1234,keep_cross_validation_predictions = TRUE,nfolds = 5)

h2o.performance(rf)
rf_result = as.data.frame(h2o.predict(object = rf,newdata = test.h2o))


gbm.model <- h2o.gbm(y=dependent_var, training_frame = train.h2o, ntrees = 1000, learn_rate = 0.01, seed = 1234,nfolds = 5,keep_cross_validation_predictions = TRUE)

h2o.performance (gbm.model)

predict.gbm <- as.data.frame(h2o.predict(gbm.model, test.h2o))

#PredictFor_model_data prediction via lm

rf = h2o.randomForest(y = dependent_var,training_frame = trainTill2015ForH2o.h20,ntrees = 1000,mtries = 10,seed = 1234,keep_cross_validation_predictions = TRUE,nfolds = 5)
gbm.model <- h2o.gbm(y=dependent_var, training_frame = trainTill2015ForH2o.h20, ntrees = 1000, learn_rate = 0.01, seed = 1234,nfolds = 5,keep_cross_validation_predictions = TRUE)

#xg = h2o.xgboost(y=dependent_var, training_frame = trainTill2015ForH2o.h20, learn_rate = 0.01, seed = 1234,nfolds = 5,keep_cross_validation_predictions = TRUE)

predictForSumbmission <- as.data.frame(h2o.predict(rf, predict.h20))
write.csv(predictForSumbmission,"submission.csv")

h2o.shutdown()
####Another method which worked 
library(randomForest)
library(rpart)
library(caret)


modelRF = randomForest(final_train ~ . , trainimputed_withouttar,ntree = 500,mtry = 10)
result = predict(object = modelRF,newdata = PredictFor_model_data)
modelRF$importance


model_dt = rpart(final_train ~ . , trainimputed_withouttar)
result_dt = predict(object = modelRF,newdata = PredictFor_model_data)


#Writing Prediciton
write.csv(result,"prediction.csv")

