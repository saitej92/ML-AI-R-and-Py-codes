{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model, Sequential\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading the file\n",
    "df=pd.read_csv('C:/My Backup/Practice Folder/Insofe Course/Material/Cute Test Preparation/Test 4/cse7321c-cute04-team-8_john_sai-_narayan-master/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dropping all rows with blanks in column converse\n",
    "df=df.dropna(axis=0, how='any')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dropping all rows with JUNK in column categories\n",
    "df = df[df[\"categories\"] != 'JUNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dropping all rows with JUNK in column sub_categories\n",
    "df = df[df[\"sub_categories\"] != 'JUNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs2=df[\"converse\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57234, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs2)\n",
    "vocab_size = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "encoded_docs2 = t.texts_to_sequences(docs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  410   18 ...,    0    0    0]\n",
      " [  31  118    1 ...,    0    0    0]\n",
      " [   2    1  120 ...,    0    0    0]\n",
      " ..., \n",
      " [1985 1165   22 ...,    0    0    0]\n",
      " [4319 1549   39 ...,    0    0    0]\n",
      " [ 240    7 1115 ...,    0    0    0]]\n",
      "Shape of data tensor: (57234, 1000)\n",
      "Shape of label tensor: (57234, 5)\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 1000 words\n",
    "max_length = 1000\n",
    "padded_docs = pad_sequences(encoded_docs2, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "\n",
    "#labels=to_categorical(np.asarray(df.categories.tolist()))\n",
    "labels = pd.get_dummies(df['categories'])\n",
    "print('Shape of data tensor:', padded_docs.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_combine=np.concatenate((padded_docs, labels), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11446"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_validation_samples = int(0.2 * padded_docs.shape[0])\n",
    "num_validation_samples"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filecombine = pd.DataFrame(df_combine).to_csv('C:\\\\My Backup\\\\Practice Folder\\\\Insofe Course\\\\Material\\\\Cute Test Preparation\\\\Test 4\\\\cse7321c-cute04-team-8_john_sai-_narayan-master\\\\df_combine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "#Splitting into train & validation\n",
    "num_validation_samples = int(0.2 * padded_docs.shape[0])\n",
    "\n",
    "x_train = padded_docs[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = padded_docs[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print('Preparing embedding matrix.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filextrain = pd.DataFrame(x_train).to_csv('C:\\\\My Backup\\\\Practice Folder\\\\Insofe Course\\\\Material\\\\Cute Test Preparation\\\\Test 4\\\\cse7321c-cute04-team-8_john_sai-_narayan-master\\\\xtraintemp.csv')\n",
    "#filexval = pd.DataFrame(x_val).to_csv('C:\\\\My Backup\\\\Practice Folder\\\\Insofe Course\\\\Material\\\\Cute Test Preparation\\\\Test 4\\\\cse7321c-cute04-team-8_john_sai-_narayan-master\\\\xval.csv')\n",
    "#fileytrain = pd.DataFrame(y_train).to_csv('C:\\\\My Backup\\\\Practice Folder\\\\Insofe Course\\\\Material\\\\Cute Test Preparation\\\\Test 4\\\\cse7321c-cute04-team-8_john_sai-_narayan-master\\\\ytraintemp.csv')\n",
    "#fileyval = pd.DataFrame(y_val).to_csv('C:\\\\My Backup\\\\Practice Folder\\\\Insofe Course\\\\Material\\\\Cute Test Preparation\\\\Test 4\\\\cse7321c-cute04-team-8_john_sai-_narayan-master\\\\yval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57234"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('C:/My Backup/Practice Folder/Insofe Course/Material/Day 29/glove.6B/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,\n",
    "                    100,\n",
    "                    input_length=1000,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False\n",
    "                    ))\n",
    "model.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
    "model.add(Dense(50, activation = 'relu'))\n",
    "#model.add(Dense(len(labels), activation='softmax'))\n",
    "model.add(Dense(5, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "              patience=5, min_lr=0.00001, verbose=1, epsilon=0.001)\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=1, mode='auto')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45788 samples, validate on 11446 samples\n",
      "Epoch 1/15\n",
      "45788/45788 [==============================] - 22s - loss: 1.3060 - acc: 0.4859 - val_loss: 1.2169 - val_acc: 0.5185\n",
      "Epoch 2/15\n",
      "45788/45788 [==============================] - 21s - loss: 1.1000 - acc: 0.5854 - val_loss: 1.1248 - val_acc: 0.5792\n",
      "Epoch 3/15\n",
      "45788/45788 [==============================] - 22s - loss: 1.0053 - acc: 0.6263 - val_loss: 1.0781 - val_acc: 0.6074\n",
      "Epoch 4/15\n",
      "45788/45788 [==============================] - 21s - loss: 0.9447 - acc: 0.6466 - val_loss: 1.0267 - val_acc: 0.6237\n",
      "Epoch 5/15\n",
      "45788/45788 [==============================] - 21s - loss: 0.9013 - acc: 0.6631 - val_loss: 0.9920 - val_acc: 0.6393\n",
      "Epoch 6/15\n",
      "45788/45788 [==============================] - 21s - loss: 0.8676 - acc: 0.6781 - val_loss: 0.9700 - val_acc: 0.6507\n",
      "Epoch 7/15\n",
      "45788/45788 [==============================] - 22s - loss: 0.8418 - acc: 0.6892 - val_loss: 0.9450 - val_acc: 0.6623\n",
      "Epoch 8/15\n",
      "45788/45788 [==============================] - 22s - loss: 0.8223 - acc: 0.6972 - val_loss: 0.9280 - val_acc: 0.6699\n",
      "Epoch 9/15\n",
      "45788/45788 [==============================] - 22s - loss: 0.8068 - acc: 0.7036 - val_loss: 0.9348 - val_acc: 0.6673\n",
      "Epoch 10/15\n",
      "45788/45788 [==============================] - 22s - loss: 0.7945 - acc: 0.7069 - val_loss: 0.9276 - val_acc: 0.6705\n",
      "Epoch 11/15\n",
      "45788/45788 [==============================] - 21s - loss: 0.7840 - acc: 0.7109 - val_loss: 0.9099 - val_acc: 0.6753\n",
      "Epoch 12/15\n",
      "45788/45788 [==============================] - 21s - loss: 0.7757 - acc: 0.7142 - val_loss: 0.9000 - val_acc: 0.6810\n",
      "Epoch 13/15\n",
      "45788/45788 [==============================] - 22s - loss: 0.7684 - acc: 0.7167 - val_loss: 0.8881 - val_acc: 0.6810\n",
      "Epoch 14/15\n",
      "45788/45788 [==============================] - 22s - loss: 0.7622 - acc: 0.7198 - val_loss: 0.8845 - val_acc: 0.6851\n",
      "Epoch 15/15\n",
      "45788/45788 [==============================] - 22s - loss: 0.7565 - acc: 0.7215 - val_loss: 0.8777 - val_acc: 0.6852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x271841b8630>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(x_train), np.array(y_train), batch_size=64, epochs=15, validation_data=(np.array(x_val), np.array(y_val)), callbacks=[reduce_lr, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 100)          3928900   \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 10001     \n",
      "=================================================================\n",
      "Total params: 3,938,901\n",
      "Trainable params: 10,001\n",
      "Non-trainable params: 3,928,900\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "result123=pd.DataFrame(result).to_csv('C:\\\\My Backup\\\\Practice Folder\\\\Insofe Course\\\\Material\\\\Cute Test Preparation\\\\Test 4\\\\cse7321c-cute04-team-8_john_sai-_narayan-master\\\\result123.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
